## TensorFlow中的激励函数

神经网络中激励函数的作用通俗上讲就是将多个线性输入转换成非线性的关系。如果不使用激励函数，神经网络的每层都只是做线性变换，即使是多层输入叠加后也还是线性变换。通过激励函数引入非线性因素后，使得神经网络的表示能力更强。

1. sigmoid函数

   ![img](https://static.oschina.net/uploads/space/2018/0226/183022_BMeZ_876354.png)

这应该是神经网络中使用最频繁的激励函数了。它把一个实数压缩至0到1之间，当输入的数字非常大的时候，结果会接近1，当输入非常大的负数时，则会得到接近0的结果。在早期的神经网络中使用得非常多，因为它很好地解释了神经元受到刺激后是否被激活和向后传递的场景（0：几乎没有被激活，1：完全被激活）。不过近几年在深度学习的应用中比较少见到它的身影，因为使用sigmoid函数容易出现梯度弥散或者梯度饱和。当神经网络的层数很多时，如果每一层的激励函数都采用sigmoid函数的话，就会产生梯度弥散的问题，因为利用反向传播更新参数时，会乘以它的导数，所以会一直减小。如果输入的是比较大或者比较小的数（例如输入100，经Sigmoid函数后结果接近于1，梯度接近于0），会产生饱和效应，导致神经元类似于死亡状态。

2. tanh函数

   ![img](https://static.oschina.net/uploads/space/2018/0226/183108_CSSq_876354.png)

tanh函数将输入压缩至-1到1之间。和sigmoid函数类似。

3. ReLU函数

   ![img](https://static.oschina.net/uploads/space/2018/0226/183117_AQxk_876354.png)

ReLU是修正线性单元（The Rectified Linear Unit）的简称，近些年来在深度学习中使用得很多，可以解决梯度弥散问题，因为它的导数等于1或者就是0。相对于sigmoid和tanh激励函数，对ReLU求梯度非常简单，计算也很简单，可以非常大程度地提升随机梯度下降的收敛速度。（因为ReLU是线性的，而sigmoid和tanh是非线性的）。
但ReLU的缺点是比较脆弱，随着训练的进行，可能会出现神经元死亡的情况，例如有一个很大的梯度流经ReLU单元后，那权重的更新结果可能是，在此之后任何的数据点都没有办法再激活它了。如果发生这种情况，那么流经神经元的梯度从这一点开始将永远是0。也就是说，ReLU神经元在训练中不可逆地死亡了。

4. Leaky ReLU函数

   ![img](https://static.oschina.net/uploads/space/2018/0226/183125_dDUy_876354.png)

Leaky ReLU主要是为了避免梯度消失，当神经元处于非激活状态时，允许一个非0的梯度存在，这样不会出现梯度消失，收敛速度快。它的优缺点跟ReLU类似。

5. ELU函数

   ![img](https://static.oschina.net/uploads/space/2018/0226/183133_XffM_876354.png)

ELU在正值区间的值为x本身，这样减轻了梯度弥散问题（x>0区间导数处处为1），这点跟ReLU、Leaky ReLU相似。而在负值区间，ELU在输入取较小值时具有软饱和的特性，提升了对噪声的鲁棒性