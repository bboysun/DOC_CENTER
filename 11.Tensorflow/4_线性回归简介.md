## 线性回归简介

#### 一、概念

线性回归 Linear Regression 是一种通过属性的线性组合来进行预测的线性模型，其目的就是找到一条直线或者一个平面或者更高维的超平面，使得预测值与真实值之间的误差最小化。

#### 二、线性拟合

利用TensorFlow我们可以先做一个简单的线性拟合Demo，来感受一下什么是线性。

```python
import numpy as np
import tensorflow as tf

# create data
x_data = np.random.rand(100).astype(np.float32)
y_data = x_data * 0.1 + 0.3

### create tensorflow structure start ###

Weights = tf.Variable(tf.random_uniform([1],-1.0,1.0))
biases = tf.Variable(tf.zeros([1]))

y = Weights * x_data + biases

loss = tf.reduce_mean(tf.square(y-y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

init = tf.initialize_all_variables()
### create tensorflow structure end ###

### go go go ###
sess = tf.Session()
sess.run(init)   # active init is very important

for step in range(201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(Weights), sess.run(biases))

 ###输出结果###
0 [-0.31578588] [0.81651604]
20 [-0.06380707] [0.39579657]
40 [0.04983678] [0.32933614]
60 [0.08463833] [0.3089837]
80 [0.09529574] [0.30275112]
100 [0.09855939] [0.3008425]
120 [0.09955883] [0.300258]
140 [0.0998649] [0.30007902]
160 [0.09995861] [0.3000242]
180 [0.09998732] [0.30000743]
200 [0.09999612] [0.30000228]
```

上面的Demo通过梯度下降优化器，不停地优化权重和偏差，通过200次的迭代最终拟合到权重是0.099，偏差是0.300，与真实的线性方程的参数失之毫厘。

#### 三、梯度下降

线性回归就是在大量杂乱数据中（其实这些数据是杂乱分布在一个线性方程周围的，不然用线性回归做模型就会失去真正的模型的意义），通过各种各种优化函数，也就是TensorFlow为我们提供了很多的优化器，不断的迭代拟合找到一些最优的权重和偏差参数，最终生成我们想要的一个线性函数即线性模型。

那么问题来了，梯度下降又是什么鬼？具体公式就不说了，简单理解就是求导（也可以理解为斜率），当导数的绝对值最小，表明我们找到一个最优值（最大或者最小值）。

梯度下降有个缺点，就是需要把所有的样本全部都加起来，一次性修改梯度。好处就是简单，容易理解。但是假如训练样本数据是海量的话，这样就会很耗内存和计算量。这个时候，我们就会想到从样本集合中随机选择一小部分样本用同样的方式来估计得到的梯度值，这就是随机梯度下降。

原理公式推导就不说了，这个有点头大。。。我们此次的目的是理解应用:smiley:

